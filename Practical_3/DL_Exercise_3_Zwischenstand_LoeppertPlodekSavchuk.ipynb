{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise Sheet 3 - Covid-19 Classification using Transfer Learning\n\n- Deep Learning for Computer Vision - Winter Term 2023/24\n- Organizers: Anwai Archit, Sushmita Nair, Constantin Pape\n- Tutors: Ahsan Ali, Anwai Archit, Lukas Friedrich, Piklu Mallick, Sushmita Nair, Ayush Paliwal\n- Due date: **Tuesday**, Dec 5, before 10:00\n\n## Time Required to Solve this Exercise Sheet\n\nAs you will train deep CNNs on this exercise sheet, model training will require an increased amount of time. So we recommend to start working on this sheet early.\n\n## Topic\n\nIn this exercise, you will solve an image classification task from medical imaging: classification in Chest X-Ray images into patients with Covid-19, Pneunomia or Healthy. We will use a subset of the dataset from a kaggle challenge for this. (https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia)\n\nThe main focus of this exercise is transfer learning and you will approach the classification task with three different approaches:\n\n    Training ResNets from scratch. (Note that we will use the ResNet implementation from torchvision throughout the exercise).\n    Training ResNets pretrained on ImageNet.\n    Training ResNets pretrained on RadImageNet, a large radiology dataset.\n\nIn addition you can combine these approaches with other methods to improve the model at the end and upload your best solution on a hold-out test set. This is explained in more details at the end of the exercise.","metadata":{}},{"cell_type":"markdown","source":"### Hints\n\nIn the first part of the exercise you will train ResNets from scratch, analyze the effects of deeper models with small training data and use data augmentations. In the second part, you will solve the same task using pretrained ResNets (from ImageNet - pretrained on natural images; and RadImageNet - pretrained on the medical imaging domain).\n\nTo understand the background of this exercise, you can:\n- Review the lectures\n    - Lecture 3 on CNNs\n    - Lecture 4 on Transfer Learning and Augmentaion\n- Check out the [RadImageNet publication](https://doi.org/10.1148/ryai.210315).\n\nAt the end of the exercise you should further improve your model. You can draw upon a number of techniques we discussed for improving model performance. The predictions from your best model on a hold-out test set should be uploaded together with the exercise. More explanation is given at the end of the exercise sheet.\n\n_Do not hesitate to ask questions and ideally discuss them with the fellow students on Rocket Chat! We will monitor the channel to provide you help if your discussions get stuck._","metadata":{}},{"cell_type":"markdown","source":"## IMPORTANT SUBMISSION INSTRUCTIONS\n- **You need to answer all the questions in written form**\n- When you are done, download the notebook from Kaggle and **rename** it to `Tutorial_<X>_<surname1>_<surname2>_<surname3>.ipynb`\n- For the final submission:\n    - Submit the **Jupyter Notebook** (.ipynb file). Upload them on `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Notebook`\n    - Submit the **Challenge Results** (.csv file) (for the unlabeled images, namely `unknown`). Upload them on `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Challenge Results`\n- Make only one submission of the exercise and results per group.\n- The deadline is strict.\n- You have to present the exercise in the tutorials. We have a high ratio of students to tutors, so please decide which team member presents which part beforehand.\n\nImplementation\n- Do not change the cells which are marked as `DO NOT CHANGE`, similarly write your solution into the cells marked with TODOs and answer the **questions** asked. In addition to the python packages loaded below, you are allowed to use any packages you want.","metadata":{}},{"cell_type":"markdown","source":"**Importing required libraries:**\n\nWe have provided the most essential libraries to be used for the exercise. Feel free to add modules as per your requirement.","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom IPython.display import FileLink\n\nimport numpy as np\nimport imageio.v3 as imageio\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\n!pip install barbar","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:11:25.340091Z","iopub.execute_input":"2023-12-01T15:11:25.340778Z","iopub.status.idle":"2023-12-01T15:11:36.638879Z","shell.execute_reply.started":"2023-12-01T15:11:25.340722Z","shell.execute_reply":"2023-12-01T15:11:36.637727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the device (make sure device returns \"cuda\" to use of the GPUs on kaggle)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:11:36.641093Z","iopub.execute_input":"2023-12-01T15:11:36.641397Z","iopub.status.idle":"2023-12-01T15:11:36.646715Z","shell.execute_reply.started":"2023-12-01T15:11:36.641370Z","shell.execute_reply":"2023-12-01T15:11:36.645807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting the Required Functionalities\n\nWe first download and import `ex3_utils.py`, which in which functions for training evaluation etc. are already implemented (similar to the previous exercise). You can download this file and inspect it on your computer to understand the functions it contains.\n\nYou can either download it from google drive as here or from Stud.IP (it's provided there in the same folder as this notebook.)","metadata":{}},{"cell_type":"code","source":"# DO NOT CHANGE\n# Download `ex3_utils.py` from Google Drive in Kaggle\n!conda install -y gdown\n\n# File Location - https://drive.google.com/file/d/1VfnvYTwNbDdwFArYZqZuqhkmw9d39IgQ/view?usp=sharing (we only need the id)\n!gdown 1VfnvYTwNbDdwFArYZqZuqhkmw9d39IgQ","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:11:36.647785Z","iopub.execute_input":"2023-12-01T15:11:36.648032Z","iopub.status.idle":"2023-12-01T15:14:12.321761Z","shell.execute_reply.started":"2023-12-01T15:11:36.648010Z","shell.execute_reply":"2023-12-01T15:14:12.320765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT CHANGE\n# Import `ex3_utils.py`\nimport ex3_utils as utils","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:14:12.325102Z","iopub.execute_input":"2023-12-01T15:14:12.325463Z","iopub.status.idle":"2023-12-01T15:14:12.330989Z","shell.execute_reply.started":"2023-12-01T15:14:12.325432Z","shell.execute_reply":"2023-12-01T15:14:12.330171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation\n\nWe download the data from google drive and unzip the data in `\"/kaggle/working/covid19\"`.","metadata":{}},{"cell_type":"code","source":"# DO NOT CHANGE\n# Downloading the \"Chest X-ray (Covid-19 & Pneumonia)\" data from owncloud\n!wget https://owncloud.gwdg.de/index.php/s/dcvhmxtksDDDtK8/download -O covid19-xray.zip\n\n# Unzipping the dataset\n!unzip -q \"/kaggle/working/covid19-xray.zip\"","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:14:12.331937Z","iopub.execute_input":"2023-12-01T15:14:12.332169Z","iopub.status.idle":"2023-12-01T15:18:20.702806Z","shell.execute_reply.started":"2023-12-01T15:14:12.332147Z","shell.execute_reply":"2023-12-01T15:18:20.701801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT CHANGE\n# Setting the path to covid-19 dataset\nroot_dir = \"/kaggle/working/\"\ndata_folder = os.path.join(root_dir, \"covid19\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:20.704166Z","iopub.execute_input":"2023-12-01T15:18:20.704444Z","iopub.status.idle":"2023-12-01T15:18:20.710598Z","shell.execute_reply.started":"2023-12-01T15:18:20.704418Z","shell.execute_reply":"2023-12-01T15:18:20.709678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data exploration**\n\nThe data is stored in the root folder `covid19` (in `\"/kaggle/working\"`) and contains four subfolders, namely `train`, `val`, `test` and `unknown` with training / validation / testing split and the hold-out test set, respectively. Each of these (except `unknown`) contains subfolders with the images for the respective classes. As a first step, we will visualize some of the images and labels from the training data.\n\nWe will not use the `unknown` directory for training or evaluation, but will only use it for prediction with the best model at the end.","metadata":{}},{"cell_type":"code","source":"# DO NOT CHANGE\n# Checking the number of classes\ntrain_class_dirs = glob(os.path.join(data_folder, \"train\", \"*\"))\nval_class_dirs = glob(os.path.join(data_folder, \"val\", \"*\"))\ntest_class_dirs = glob(os.path.join(data_folder, \"test\", \"*\"))\nassert len(train_class_dirs) == len(val_class_dirs) == len(test_class_dirs) == 3","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:20.711984Z","iopub.execute_input":"2023-12-01T15:18:20.712249Z","iopub.status.idle":"2023-12-01T15:18:20.722251Z","shell.execute_reply.started":"2023-12-01T15:18:20.712225Z","shell.execute_reply":"2023-12-01T15:18:20.721497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT CHANGE\n# Checking the expected structure of all images\nimage_paths = glob(os.path.join(data_folder, \"**\", \"*.jpg\"), recursive=True)\nassert len(image_paths) == (450 + 150 + 225 + 5607), len(image_paths)  # 450 train samples, 150 val samples, 225 test samples, 5607 unlabeled samples (hold-out test)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:20.723324Z","iopub.execute_input":"2023-12-01T15:18:20.723593Z","iopub.status.idle":"2023-12-01T15:18:20.761157Z","shell.execute_reply.started":"2023-12-01T15:18:20.723565Z","shell.execute_reply":"2023-12-01T15:18:20.760509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot samples from each class in the training set:","metadata":{}},{"cell_type":"code","source":"# Classes available for the task\nclasses = [\"COVID19\", \"NORMAL\", \"PNEUMONIA\"]\nnum_classes = 3\n\n# TODO: YOUR SOLUTION HERE\n\nimport random\nimport matplotlib.image as mpimg\n\ndef plot_samples_per_class(class_dirs, num_samples=3):\n    for class_dir in class_dirs:\n        class_name = os.path.basename(class_dir)\n        print(f\"Class: {class_name}\")\n        \n        # Get random samples from the class\n        class_samples = random.sample(image_paths, num_samples)\n        \n        # Plot the samples\n        plt.figure(figsize=(15, 5))\n        for i, sample_path in enumerate(class_samples):\n            plt.subplot(1, num_samples, i + 1)\n            img = mpimg.imread(sample_path)\n            plt.imshow(img, cmap='gray')  # Assuming images are grayscale\n            plt.title(f\"Sample {i + 1}\")\n            plt.axis('off')\n        \n        plt.show()\n\nplot_samples_per_class(train_class_dirs)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:20.762136Z","iopub.execute_input":"2023-12-01T15:18:20.762400Z","iopub.status.idle":"2023-12-01T15:18:22.420826Z","shell.execute_reply.started":"2023-12-01T15:18:20.762377Z","shell.execute_reply":"2023-12-01T15:18:22.419896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You should see x-ray images from each class and their corresponding labels in their titles. To reflect on our understanding of the dataset, please answer the following **questions**:\n- How many classes are provided in the datasets?\n- Are the images balanced along all the classes in the respective data splits?","metadata":{}},{"cell_type":"markdown","source":"Let's set the directories for the respective datasets","metadata":{}},{"cell_type":"code","source":"# Splits for the dataset \ntrain_dir = os.path.join(data_folder, \"train\")\nval_dir = os.path.join(data_folder, \"val\")\ntest_dir = os.path.join(data_folder, \"test\")\nunknown_dir = os.path.join(data_folder, \"unknown\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:22.424416Z","iopub.execute_input":"2023-12-01T15:18:22.425141Z","iopub.status.idle":"2023-12-01T15:18:22.429571Z","shell.execute_reply.started":"2023-12-01T15:18:22.425112Z","shell.execute_reply":"2023-12-01T15:18:22.428761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we obtain the statistics (mean and standard deviation) from the training dataset to be used for normalizing the datasets","metadata":{}},{"cell_type":"code","source":"# DO NOT CHANGE\ntrain_images = []\nfor class_name in classes:\n    train_images.extend(glob(os.path.join(train_dir, class_name, \"*.jpg\")))\n\nto_tensor = torchvision.transforms.ToTensor()\nimgs = torch.stack([to_tensor(imageio.imread(im)) for im in train_images])\nprint(imgs.shape)\n\n# And then compute the mean and standard deviation independently for the image channels.\n# (The channels are stored in dim=1, by excluding this below we achieve this.)\nmean = torch.mean(imgs, dim=(0, 2, 3))\nstd = torch.std(imgs, dim=(0, 2, 3))\n\n# Delete the images again in order to save memory.\ndel imgs\n\nprint(mean)\nprint(std)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:22.430932Z","iopub.execute_input":"2023-12-01T15:18:22.431506Z","iopub.status.idle":"2023-12-01T15:18:26.164043Z","shell.execute_reply.started":"2023-12-01T15:18:22.431471Z","shell.execute_reply":"2023-12-01T15:18:26.163104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_mean = mean\ntrain_dataset_std = std","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:26.165345Z","iopub.execute_input":"2023-12-01T15:18:26.166075Z","iopub.status.idle":"2023-12-01T15:18:26.171383Z","shell.execute_reply.started":"2023-12-01T15:18:26.166025Z","shell.execute_reply":"2023-12-01T15:18:26.170476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implement PyTorch dataloader**\n\nAs a next step, we implement a `torch.utils.data.Dataset` followed by the `torch.utils.data.DataLoader` to have access to our data during training, validation and testing. In our case, the data is stored in a format that is already compatible with `torchvision.datasets.ImageFolder`.","metadata":{}},{"cell_type":"markdown","source":"To use the image data in PyTorch it first needs to be transformed. You can use the transformations from [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html) for this. Here, we need to:\n- Convert the image data to a `torch.tensor` (`transforms.ToTensor`)\n- Standardize the inputs based on their data statistics (`transforms.Normalize`)\n- Resize the images (`transforms.Resize`). Note that resizing is not strictly necessary, but will speed up training and resize the images to a size that better matches the ImageNet pretraining data.\n\nTo combine several transforms together, you can use `torchvision.transforms.Compose` by passing the list of 'transform' objects to compose.","metadata":{}},{"cell_type":"code","source":"# We resize the images to the size 256 x 256 to speed up training\nheight = width = 256\n\n# Get the usual transforms to have the inputs from dataloaders as expected\ndef get_transforms(height, width):\n    # TODO: YOUR CODE HERE\n    transform = transforms.Compose([\n        transforms.Resize((height, width)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    return transform","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:26.172633Z","iopub.execute_input":"2023-12-01T15:18:26.172985Z","iopub.status.idle":"2023-12-01T15:18:26.185594Z","shell.execute_reply.started":"2023-12-01T15:18:26.172959Z","shell.execute_reply":"2023-12-01T15:18:26.184863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch size\nbatch_size = 32  # found to be a common choice for this dataset\n\n# Datasets\n# TODO: YOUR SOLUTION HERE\n\n# Specify the image size for transformations\nheight = width = 256\n\n# Get the transformations\ntransform = get_transforms(height, width)\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform)\ntest_dataset = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n\n# Dataloaders\n# TODO: YOUR SOLUTION HERE\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:26.186795Z","iopub.execute_input":"2023-12-01T15:18:26.187053Z","iopub.status.idle":"2023-12-01T15:18:26.202727Z","shell.execute_reply.started":"2023-12-01T15:18:26.187030Z","shell.execute_reply":"2023-12-01T15:18:26.201954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's visualize the images after transformation as they are returned from the training loader.","metadata":{}},{"cell_type":"code","source":"# Function to show images\nfig = plt.figure(figsize=(10, 15))\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalise\n    npimg = img.numpy()\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Obtain random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show the images\nimshow(torchvision.utils.make_grid(images))\n\nprint(' '.join('%5s, ' % classes[labels[j]] for j in range(len(labels))))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:26.203885Z","iopub.execute_input":"2023-12-01T15:18:26.204154Z","iopub.status.idle":"2023-12-01T15:18:27.007899Z","shell.execute_reply.started":"2023-12-01T15:18:26.204131Z","shell.execute_reply":"2023-12-01T15:18:27.006962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture\n\nWe will use the ResNet implementation from torchvision, see https://pytorch.org/vision/stable/models.html, for this exercise.\n\n- We start with the smallest ResNet model, the ResNet18 (https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html)\n- And will then use deeper ResNets to explore how well larger models can be trained on a small dataset.\n- To use random weight initialization (in order to train from scratch), you can just create models without passing additional arguments like so: `torchvision.models.<MODEL_NAME>()`.","metadata":{}},{"cell_type":"markdown","source":"We need to adapt the model to fit with our purpose. For this, we must change the output dimension of the last fully-connected layer to consider the number of classes in our problem. Let's check the last layer out.","metadata":{}},{"cell_type":"code","source":"# Let's get the resnet18 architecture and understanding the backbone (in order to adapt it to our problem)\n# TODO: YOUR SOLUTION HERE\n\ndef resnet18_custom(num_classes):\n    resnet18 = torchvision.models.resnet18(pretrained=True)\n\n    # Freeze all layers except the final classification layer\n    for param in resnet18.parameters():\n        param.requires_grad = False\n\n    # Modify the output dimension of the last fully-connected layer\n    resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n\n    return resnet18\n\n\nmodel = resnet18_custom(num_classes)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.009117Z","iopub.execute_input":"2023-12-01T15:18:27.009470Z","iopub.status.idle":"2023-12-01T15:18:27.209792Z","shell.execute_reply.started":"2023-12-01T15:18:27.009435Z","shell.execute_reply":"2023-12-01T15:18:27.208915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see above that there is a component called `fc` that consists of a `Linear` layer. To make use of ResNet18 for our classification task, we need to change the `out_features` of the linear layer to the number of classes of our problem. In our case, it's `num_classes` (=3).","metadata":{}},{"cell_type":"code","source":"print(model.fc)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.210935Z","iopub.execute_input":"2023-12-01T15:18:27.211222Z","iopub.status.idle":"2023-12-01T15:18:27.216047Z","shell.execute_reply.started":"2023-12-01T15:18:27.211195Z","shell.execute_reply":"2023-12-01T15:18:27.215156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To replace the last layer (classification layer) you can use the following code. (Here we provide a stand-alone code snippet)\n\n```python\nimport torchvision\nimport torch.nn as nn\n\nmodel = torchvision.models.resnet18()\n\n# Let's replace the \"fully connected\" layer to match our expected output classes\nmodel.fc = nn.Linear(<INPUT_FEATURES>, <OUTPUT_CLASSES>)\nmodel.to(device)\n```","metadata":{}},{"cell_type":"code","source":"# Replacing the last layer(s) of ResNet18 to match our number of classes\n# TODO: YOUR SOLUTION HERE\n# model = \n\n\n# Let's define the checkpoint name where the specific model checkpoint will be saved\nmodel_name = 'resnet18_mod'\ncheckpoint_name = f'covid-19-{model_name}-from-scratch.pt'\ncheckpoint_path = os.path.join(root_dir, checkpoint_name)\nprint(\"The model checkpoint will be saved here: \", checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.217321Z","iopub.execute_input":"2023-12-01T15:18:27.217578Z","iopub.status.idle":"2023-12-01T15:18:27.227138Z","shell.execute_reply.started":"2023-12-01T15:18:27.217555Z","shell.execute_reply":"2023-12-01T15:18:27.226315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you now display the model (using `print(model)`) you should see that its last layer has been updated.","metadata":{}},{"cell_type":"markdown","source":"### Training\n\nLet's train the model for 10 epochs, using the `Adam` optimizer, `CrossEntropyLoss` as the criterion (loss function) and a learning rate scheduler (e.g. `ReduceLROnPlateau`).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\n# Clearing the GPU cache\ntorch.cuda.empty_cache()\n\n\nprint(\"Starting training from scratch with network: \", model_name)\n\n\n# TODO: YOUR SOLUTION HERE\n# Optimizer and loss configurations\ncriterion = nn.CrossEntropyLoss()  # loss function\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # optimizer\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1, verbose=True)  # learning rate scheduler\n\n\n# Initializing the early stopping of the training\nearly_stopping = utils.EarlyStopping(checkpoint_path=checkpoint_path, patience=5, verbose=True, delta=0.001)\n\n# HINT: open the 'ex3_utils.py' file to understand the arguments of 'run_training'\ntrain_accuracy, val_accuracy, train_losses, val_losses = utils.run_training(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    net=model,\n    num_epochs=num_epochs,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    early_stopping=early_stopping,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:35:42.543646Z","iopub.execute_input":"2023-12-01T15:35:42.544596Z","iopub.status.idle":"2023-12-01T15:36:13.541899Z","shell.execute_reply.started":"2023-12-01T15:35:42.544561Z","shell.execute_reply":"2023-12-01T15:36:13.540876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's see how the accuracy and loss plots look for our training.","metadata":{}},{"cell_type":"code","source":"# Training and validation accuracy plots\n# TODO: YOUR SOLUTION HERE\nutils.get_metric_plots(\n    train_metric=train_accuracy,\n    val_metric=val_accuracy,\n    metric_name=\"Accuracy\"\n)\nplt.show()\n\n\n# Training and validation loss plots\n# TODO: YOUR SOLUTION HERE\nutils.get_metric_plots(\n    train_metric=train_losses,\n    val_metric=val_losses,\n    metric_name=\"Loss\"\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:36:27.445758Z","iopub.execute_input":"2023-12-01T15:36:27.446417Z","iopub.status.idle":"2023-12-01T15:36:27.995908Z","shell.execute_reply.started":"2023-12-01T15:36:27.446384Z","shell.execute_reply":"2023-12-01T15:36:27.995024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation on the test set\n\nLet's check the test accuracy and confusion matrix. All the required functionality is already provided in `ex3_utils.py` and you just need to read the corresponding functions to understand how to call them here.","metadata":{}},{"cell_type":"code","source":"# Loading the best model for inference (NOTE: you should load the checkpoints to the expected model architecture, else you might get some mismatch errors)\nbest_model = YourModelClass()\nbest_model.load_state_dict(torch.load(checkpoint_path))\nbest_model.to(device)\n# TODO: YOUR SOLUTION HERE\n\n\n# Provide the testing dataset wrapped in a dataloader to check for inference\n# TODO: YOUR SOLUTION HERE\ncorrect, total = utils.test_evaluation(...)\nprint(f'Accuracy of {checkpoint_path} on the Test Images: %d %%' % (100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.276696Z","iopub.status.idle":"2023-12-01T15:18:27.277031Z","shell.execute_reply.started":"2023-12-01T15:18:27.276877Z","shell.execute_reply":"2023-12-01T15:18:27.276892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the confusion matrix for the test dataset\n# TODO: YOUR SOLUTION HERE\ncm = utils.get_confusion_matrix(...)\n\n\n# See the precision, recall and accuracy per class for the test dataset\n# TODO: YOUR SOLUTION HERE\nutils.check_precision_recall_accuracy(...)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.278193Z","iopub.status.idle":"2023-12-01T15:18:27.278634Z","shell.execute_reply.started":"2023-12-01T15:18:27.278413Z","shell.execute_reply":"2023-12-01T15:18:27.278434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the confusion matrix heatmap to visually see the evaluation on the test set\n# TODO: YOUR SOLUTION HERE\nutils.visualize_confusion_matrix(...)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.280310Z","iopub.status.idle":"2023-12-01T15:18:27.280612Z","shell.execute_reply.started":"2023-12-01T15:18:27.280461Z","shell.execute_reply":"2023-12-01T15:18:27.280476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try training deeper ResNet architectures using the same hyperparameters from above:\n- Train and evaluate a ResNet34 from scratch.\n- Train and evaluate a ResNet50 from scratch.","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTIONS HERE\n\n\n# HINT: (for the workflow)\n#    - Start with training a ResNet34:\n#          - Updating the last layers (training the network from scratch)\n#          - REMEMBER: Save the respective checkpoints uniquely, and initialize the early stopping with them.\n#          - Use the expected hyperparameters for training\n#          - Observe the loss and accuracy curves for training and validation\n#    - Next, train a ResNet50 (following the same suggestion as above)\n#    - Finally, evaluate both the trained models from above on the test set and answer the questions.","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.281958Z","iopub.status.idle":"2023-12-01T15:18:27.282287Z","shell.execute_reply.started":"2023-12-01T15:18:27.282129Z","shell.execute_reply":"2023-12-01T15:18:27.282143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Questions**:\n\n1. How does the performance of the three architectures compare to each other? Are there any specific patterns you can see in the confusion matrices?\n2. Is there a correlation between the dataset size and depth of the network?\n3. Given these observations, which model would you prefer for training on a small dataset from scratch?","metadata":{}},{"cell_type":"markdown","source":"#### Now let's try with Data Augmentation\n\nA common strategy when dealing with small datasets is to add data augmentations.\n\nLet's try a few augmentations from [torchvision.transforms](https://pytorch.org/vision/main/transforms.html) that do not significantly distort the data (stay on the data manifold) and train a ResNet50. Here, we want to see if augmentations allow us to train deeper CNNs. \nCommon augmentations for data augmentation are:\n- flipping the inputs along the axes.\n- changing the brightness, contrast and saturation of the inputs.\n- adding small noise to the input.\n\nYou can try different combinations of augmentations here. You can add augmentations by updating the `transform` passed to the train dataset. You can again use `Compose` to chain augmentations.\n\nNote; you should only introduce augmentation for the training and validation datasets, **not** for the test dataset. Otherwise the evaluation between models is not consistent anymore.","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTIONS HERE\n\n# HINT:\n#    - Training a ResNet50 (incorporating data augmentation strategies)\n#    = (the training workflows are the same as above)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.283185Z","iopub.status.idle":"2023-12-01T15:18:27.283508Z","shell.execute_reply.started":"2023-12-01T15:18:27.283347Z","shell.execute_reply":"2023-12-01T15:18:27.283363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to apply the new transforms to the dataset and then use our dataloader. Let's reuse our scripts from above, now with the added data augmentation strategies.","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTION HERE\n# Datasets\ntrain_dataset = ...\nval_dataset = ...\n\n# TODO: YOUR SOLUTION HERE\n# Dataloaders\ntrain_loader = ...\nval_dataset = ...","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.284924Z","iopub.status.idle":"2023-12-01T15:18:27.285249Z","shell.execute_reply.started":"2023-12-01T15:18:27.285090Z","shell.execute_reply":"2023-12-01T15:18:27.285106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are ready to train the ResNet50 with data augmentations. Let's train for longer (30 epochs) here, with the same hyperparameters as before, and evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTION HERE","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.286803Z","iopub.status.idle":"2023-12-01T15:18:27.287127Z","shell.execute_reply.started":"2023-12-01T15:18:27.286976Z","shell.execute_reply":"2023-12-01T15:18:27.286991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Questions**:\n\n1. Does the effect of data augmentation match your expectation?\n2. Are there other kinds of data augmentation that would make sense for this dataset? Think about augmentations that \"leave the data manifold\".","metadata":{}},{"cell_type":"markdown","source":"## ImageNet\n\nThe [ImageNet project](https://www.image-net.org/) provides a large-scale dataset with natural images. There exist different version of this dataset, the largest using 14 million annotated images for image classification with over 20,000 categories. \n\nThis dataset has been used by the ImageNet Scale Visual Recognition Challenge ([ILSVRC](https://image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale.)) to benchmark classification algorithms competing to improve classification. The version fo the dataset used for this classification contains a million training images with 1,000 categeories (and corresponds to the version fo the dataset we discussed in the lecture).\n\n`torchvision.models` has a pool of neural networks, for which pretrained ImageNet weights are also available. To make use of pretrained models, we need to preprocess the images based on the mean and standard deviation of ImageNet (this step is critical!). The statistics for ImageNet are provided [here](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#:~:text=1.0%5D%20and%20then-,normalized%20using%20mean%3D%5B0.485%2C%200.456%2C%200.406%5D%20and%20std%3D%5B0.229%2C%200.224%2C%200.225%5D.,-Next)\n\n### ImageNet transfer learning for ResNet50\n\nWe will now check if transfer learning from ImageNet leads to improvements for our dataset. Here, we will focus on the ResNet50 as we have seen before that without transfer learning its performance is worse compared to smaller architectures for our dataset.\n\nThere are two different approaches for how we can fine-tune a CNN initialized with pretrained weights:\n1. Update the weights for all layers of the network. For this approach we initialize with pretrained weights but otherwise train the network as before. See this code snippet:\n   \n    ```python\n    import torchvision\n    import torch.nn as nn\n\n    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n\n    # Let's replace the \"fully connected\" layer to match our expected output classes\n    model.fc = nn.Linear(<INPUT_FEATURES>, <OUTPUT_CLASSES>)\n    model.to(device)\n\n    # Train the network as usual\n    ```\n\n2. Only update the weights of the last layer (classification layer). In this case we would \"freeze\" the pretrained network and use it as a fixed feature extractor. This is achieved by disabling parameter updates for all but the last layer of the network.\n\nWe will start with the first approach (fine-tuning the full network).","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTIONS HERE\n\n# HINT:\n#    - Training a ResNet50 (updating all the layers, transfer learning using ImageNet weights)\n#    = (the training workflows are the same as above)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.288141Z","iopub.status.idle":"2023-12-01T15:18:27.288454Z","shell.execute_reply.started":"2023-12-01T15:18:27.288295Z","shell.execute_reply":"2023-12-01T15:18:27.288311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try the second approach and finetune only the last layer.\n\nBy default all parameters of a network will be updated by gradient descent. The gradient updates can be disabled by setting `requires_grad = False`. So in order to finetune only the last layer you need to `requires_grad = False` for all other layers in the network.\n\nYou can see how the parameters are disabled for a complete model in the code snippet below. For the next exercise you need to make sure that `requires_grad` stays `True` for the last layer!\n\n```python\nfor param_name, param in model.named_parameters():\n    param.requires_grad = False\n```","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTIONS HERE\n\n# HINT:\n#    - Training a ResNet50 (updating the last layers, transfer learning using ImageNet weights)\n#    = (the training workflows are the same as above)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.289578Z","iopub.status.idle":"2023-12-01T15:18:27.289966Z","shell.execute_reply.started":"2023-12-01T15:18:27.289798Z","shell.execute_reply":"2023-12-01T15:18:27.289819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Questions**:\n\n1. Elaborate on the reason to use pretrained ImageNet weights for transfer learning (instead of training from scratch).\n2. Which transfer learning approach performs better for finetuning on our dataset? Comment on the possible reasons!\n3. Explain the best use-cases for ImageNet pretrained weights for the two different approaches (i.e. when it is a good idea to train from scratch OR for finetuning all layers OR for finetuning last layers).","metadata":{}},{"cell_type":"markdown","source":"**Comment:** When finetuning a model it may also be beneficial to use a different learning rate and change other hyperparameters compared to training from scratch. To keep the exercise simple we do not further explore these options here. If you're interested to investigate these effects you can explore different hyperparameters at the end of the exercise.","metadata":{}},{"cell_type":"markdown","source":"## RadImageNet\n\nA disadvantage of using ImageNet pretraining for medical images is that the pretraining data is very different to medical images. There are several efforts to build pretraining datasets for the medical image domain, for example [RadImageNet](https://www.radimagenet.com/).\nIt contains a pretraining dataset made up of radiology images of a comparable size to ImageNet and provides networks that were pretrained with this dataset.\n- For more details you can check out the [RadImageNet publication](https://doi.org/10.1148/ryai.210315).\n\nNote: if you use on RadImageNet weights you will have to use the statistics below for normalization (corresponding to the image statistics of the RadImageNet dataset):\n```python\n# Normalization: mean and standard deviation values for the pretrained weights on radimagenet dataset\nradimagenet_mean = (0.223, 0.223, 0.223)\nradimagenet_std = (0.203, 0.203, 0.203)\n```\n\nWe now use pretrained RadImageNet weights for the ResNet50. For this we first need to download those weights and then initialize the architecture correctly. (This is a bit more complex compared to before, so we give you the code for this below.)","metadata":{}},{"cell_type":"code","source":"# DO NOT CHANGE\n# downloading the radimagenet pretrained model weights\n# radimagenet weights - https://drive.google.com/file/d/1RHt2GnuOYlc_gcoTETtBDSW73mFyRAtR/view\n! gdown 1RHt2GnuOYlc_gcoTETtBDSW73mFyRAtR\n\n!unzip -q \"/kaggle/working/RadImageNet_pytorch.zip\" -d \"RadImageNet_Models\"","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.291760Z","iopub.status.idle":"2023-12-01T15:18:27.292091Z","shell.execute_reply.started":"2023-12-01T15:18:27.291933Z","shell.execute_reply":"2023-12-01T15:18:27.291949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT CHANGE\nradimagenet_checkpoint_path = \"/kaggle/working/RadImageNet_Models/RadImageNet_pytorch\"\n\nresnet50_ckpt = torch.load(os.path.join(radimagenet_checkpoint_path, \"ResNet50.pt\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.293295Z","iopub.status.idle":"2023-12-01T15:18:27.293615Z","shell.execute_reply.started":"2023-12-01T15:18:27.293458Z","shell.execute_reply":"2023-12-01T15:18:27.293473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT CHANGE\n# We create the backbone to intialize it with the pretrained weights from radimagenet\nclass Backbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        base_model = torchvision.models.resnet50()\n        encoder_layers = list(base_model.children())\n        self.backbone = nn.Sequential(*encoder_layers[:9])\n                        \n    def forward(self, x):\n        return self.backbone(x)\n\n\n# We build the classifier to use the features for transfer learning\nclass Classifier(nn.Module):\n    def __init__(self, num_class):\n        super().__init__()\n        self.drop_out = nn.Dropout()\n        self.linear = nn.Linear(2048, num_class)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = self.drop_out(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.294875Z","iopub.status.idle":"2023-12-01T15:18:27.295187Z","shell.execute_reply.started":"2023-12-01T15:18:27.295027Z","shell.execute_reply":"2023-12-01T15:18:27.295041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"markdown","source":"We now combine the backbone (ResNet50 pretrained on RadImageNet) and the classification layer for our dataset.\n\nTrain and evaluate this model. You can decide wheter to finetune the whole model or just the last layer.","metadata":{}},{"cell_type":"code","source":"# Let's call the backbone (the encoder excluding the last layers)\nbackbone = Backbone()\n\n# Loading the pretrained weights to the backbone\nbackbone.load_state_dict(resnet50_ckpt)\n\n# Now let's call the expected fully connected layer\nclassifier = Classifier(num_class=len(classes))\n\n# Finally, we are ready to build our model \nnet = nn.Sequential(backbone, classifier)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.297120Z","iopub.status.idle":"2023-12-01T15:18:27.297451Z","shell.execute_reply.started":"2023-12-01T15:18:27.297289Z","shell.execute_reply":"2023-12-01T15:18:27.297305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: YOUR SOLUTIONS HERE\n\n# HINT:\n#    - Training a ResNet50 (updating all / last layers, transfer learning using RadImageNet weights)\n#    = (the training workflows are the same as above)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.299120Z","iopub.status.idle":"2023-12-01T15:18:27.299451Z","shell.execute_reply.started":"2023-12-01T15:18:27.299289Z","shell.execute_reply":"2023-12-01T15:18:27.299305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Questions**:\n\n1. Comment on your choice of transfer learning approach (either finetuning last layers / all layers in the model) for RadImageNet? Why could the best approach here be different from ImageNet?","metadata":{}},{"cell_type":"markdown","source":"### Overall Results:\n\nUpdate the table with your results for all experiments from the exercise:\n\n| Model    | Training                         | Test Accuracy |\n|:-------: |:--------------------------------:|:-------------:|\n| ResNet18 | from scratch                     |               |\n| ResNet34 | from scratch                     |               |\n| ResNet50 | from scratch                     |               |\n| ResNet50 | from scratch (with augmentation) |               |\n| ResNet50 | from ImageNet (all layers)       |               |\n| ResNet50 | from ImageNet (last layers)      |               |\n| ResNet50 | from RadImageNet                 |               |","metadata":{}},{"cell_type":"markdown","source":"**Question**:\n1. Comment on the trend observed in the results from the table.\n2. Which is the overall best model? Does this match your expectations? Why/why not?\n\n**Important: please read the end of the exercise sheet and upload the predictions from your best model, also if you choose not to try out to further improve it.**","metadata":{}},{"cell_type":"markdown","source":"## (OPTIONAL): Further improve your model.\n\nYou can now try to further improve the model by using different architectures or trying some of the advanced training and inference techniques discussed in the lecture. This part of the exercise is optional, and you can try to apply as many approaches for improving your method as you would like. If you don't want to work on the optional part at all please go to the end of the exercise to submit the predictions from your best model.\n\nHere are the aproaches you can try to improve the model:\n1. Update the training hyperparameter:\n    - You can choose a better initial learning rate, or choose different options for the learning rate scheduler or early stopping.\n    - The most important parameter is probably the learning rate; if you want to improve it then train only for a short time and compare different values. (Remember lecture 2 and the first exercise). \n2. Try other architectures provided in `torchvision.models`, which implements further CNN architectures (and their respective ImageNet pretrained weights). For example `DenseNet` or `EfficientNet` could yield better results than ResNets.\n    - `torchvision.models` also offers vision transformer architectures (https://arxiv.org/abs/2010.11929). We will cover those later in the lecture, but if you want you can try them here as well. See the snippet at the end of this section for details. \n3. Try advanced data augmentation strategies, which change the data manifold severly and can boost performance. For example MixUp or CutMix. `torchivsion` already implements them, see [data augmentation with torchvision](https://pytorch.org/vision/stable/transforms.html) for details.\n4. Try test-time data augmentation. We have covered this idea in the lecture. To use it you don't need to change the training at all, but just update the model prediction during testing. Fot this you can either update the functionality in `ex3_utils.py` or implement a wrapper function or class around the model that implements the test-time data augmentation logic.\n5. Try model ensembling: combine the predictions of different models on the test set or use an implicit averaging approach like polyak averaging. For this approaches you may need to update the training and/or prediction functions from `ex3_utils.py`.\n\nHere's a snippet that shows how to use vision transformers from torchvision:\n\n```python\n# replacing the respective layers of the \"transformer-based networks\" to match our number of classes\nnet = torchvision.models.vit_b_16(pretrained=True)\nnet.heads.head = nn.Linear(768, num_classes)\nnet.to(device)\n```","metadata":{}},{"cell_type":"markdown","source":"**Hint**: if you want to systematically improve your model you should proceed as follows:\n- Decide on the base architecture and how to initialize it.\n    - Either choose the best model you according to the exercise so far or try if advanced architectures from `torchvision` bring an improvement and continue with one of them.\n- Optimize the training hyperparameters (learning rate, scheduling, early stopping).\n- Investigate advanced techniques:\n  - Training with more or advanced data augmentation.\n  - Test-time data augmentation\n  - Model ensembling\n\nYou can divide some of these tasks up among your group and train the best model by combining the best settings you have found for the individual steps.","metadata":{}},{"cell_type":"markdown","source":"## Train your best model","metadata":{}},{"cell_type":"code","source":"# TODO: IMPLEMENT AND TRAIN THE BEST MODEL HERE","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.300283Z","iopub.status.idle":"2023-12-01T15:18:27.300576Z","shell.execute_reply.started":"2023-12-01T15:18:27.300429Z","shell.execute_reply":"2023-12-01T15:18:27.300443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission:\n\n- The `unknown` set corresponds to a hold-out test set (with unlabeled images). Such unseen test data is common in machine learning challenges to ensure an objective comparison of different methods and to test how well these solutions would generalize to real data.\n- Submit the predictons from your best model together with your exercise solution.\n    - Upload the solutions to `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Challenge Results`.\n    - Your submission should be called `Tutorial_X_Results_surname1_surname2_surname3.csv`. The expected file format is described below and we provide a function that generates these results for you already.\n- **Please submit the results from your best model (or the model you expect to perform best). If you don't wor on the optional part of the exercise then submit the best result from the models you have trained so far.**\n    - The group with the best submission will get a small prize ;-) \n\n### Expected Submission Format\n\nBefore submitting your results, please make sure that they are in the below mentioned format:\n- `Tutorial_X_Results_<surname1>_<surname2>_<surname3>.csv`\n    - patient_\\<ID-1>.jpg | \\<CLASS-1>\n    - patient_\\<ID-2>.jpg | \\<CLASS-2> <br>\n    . <br>\n    . <br>\n    . <br>\n    - patient_\\<ID_n>.jpg | \\<CLASS-[1-3]>\n \nWe have prepared a function that generates these predictions for you (see below).","metadata":{}},{"cell_type":"code","source":"# TODO: YOUR SOLUTION HERE\nfilename = \"Tutorial_X.csv\"\n\n# Function generating predictions (in a csv file) for the respective images in the hold-out test set (`unknown` folder)\n# Note: if you want to use test-time-augmentation or model ensembling you may need to update this function.\n# If you don't want to update it you can also create a new class that implements a wrapper around the model(s)\n# that implements the augmentation or ensembling logic.\nutils.predict_unknown(net, height, width, train_dataset_mean, train_dataset_std, unknown_dir, device, filename)\n\n# Download link is automatically generated for the final results generated\nFileLink(filename)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T15:18:27.301413Z","iopub.status.idle":"2023-12-01T15:18:27.301757Z","shell.execute_reply.started":"2023-12-01T15:18:27.301574Z","shell.execute_reply":"2023-12-01T15:18:27.301590Z"},"trusted":true},"execution_count":null,"outputs":[]}]}